\documentclass[letterpaper,12pt]{article}

\usepackage[margin=1.2in]{geometry}
%\usepackage{supertabular}
%\usepackage{setspace}
%\doublespacing
\usepackage{graphicx}
%\usepackage{hyperref}

\newcommand{\company}[1]{\verb|#1|}
\newcommand{\file}[1]{\texttt{#1}}
\newcommand{\program}[1]{\texttt{#1}}
\newcommand{\software}[1]{\verb|#1|}
\newcommand{\project}[1]{\texttt{#1}}
\newcommand{\boldsymbol}[1]{\mathbf{#1}}
\newcommand{\comment}[1]{ [ \textit{#1} ] }

\newcommand{\DD}{\mathcal{D}}
\newcommand{\WW}{\mathcal{W}}
\newcommand{\TT}{\mathcal{T}}

\author{Ivan Savov}
\title{ {\LARGE Latent Dirichlet Ideas } }

\begin{document}
\maketitle

\abstract{

    During the last semester I worked on a small research project on the LDA topic
    model on a dataset of scientific documents.
    %
    I feel my project has just scratching the surface of what is a fascinating subject which
    has plenty of interesting research topics to be explored.
    I also want to develop \emph{practical} applications and demos that show what
    LDA can do.
}

\ \\
\noindent {\bf keywords: } LDA, topic model, Gibbs sampling, applications, hierarchical
    % web service, startup


\section{Introduction}

	Topic models have received a lot of attention recently in the machine learning 
	community \cite{Blei2003,Blei2009}.
	This simple graphical model based on the multinomial distribution for word frequencies 
	and its prior the Dirichlet probability density function has been acclaimed for
	its ability to magically find groups of words that seem to belong to the same subject matter.
	
	
	The original model has been studied by several authors, and different 
	techniques of inferring the probability distributions have been developed
	like collapsed variational Bayes \cite{teh2007collapsed}, Gibbs sampling \cite{porteous2008fast} and others \cite{Teh2009}.
	Computational considerations have also been studied with several results
	about modifying the inference algorithms to scale \cite{newman2006scalable,newman2007distributed} and to fit 
	different hardware paradigms \cite{masada2009accelerating, yan-parallel}.
	
	The basic topic model of LDA has also been extended to involve hierarchical 
	topic structures (hLDA \cite{blei2004hierarchical}), authors and citations \cite{rosen2004author},  
	and many other variations \cite{williamson2009focused,mimno2007expertise,boyd2007topic}.
	These LDA-inspired models are often more computationally intensive to learn, 
	but often provide a natural match to a specific task.
	
	On the applications side, we have had JSTOR search and categorizations \cite{Blei2009},
	subject classification of newsgroups data and Google News. 
	There hasn't been much research conducted on large datasets to my knowledge. 
	No one has learned a topic model for the whole of wikipedia for example despite the
	fact that 5GB of text data (the current approximate size of wikipedia) can probably
	be crunched using a small number of machines.
	
	Given my fascination with topics models, I feel like I should take a more
	active approach and actually code something up and run experiments 
	instead of passively reading papers in order to keep up with what others have done.

	More specifically, here is a list of tasks that I would like to accomplish
	during the next year.
	
	\begin{enumerate}
		\item	Understand better the underlying principles 
				that make LDA work. [arbilon, exp family, ]
		\item	Explore the "topic landscape" defined between
				topics by the K-L distance ${KL}(p_{t_1}(w\in Dict),\ p_{t_2}(w \in Dict))$.
		\item	Explore consistency of learned topics over multiple runs on the same
				data. (quality of matching between topics of different runs based on K-L distance)
		\item	Explore the topic landscape defined between topics from runs with different
				number of topics, i.e. which subset of topics from a $T=100$ run are most 
				similar to the first topic from a run with $T=20$. 
		\item	Produce a python library for automating LDA
				experiments (liblda).
				(preprocessing, word counts, dictionary generation, 
				pluggable inference algorithms in C).
		\item	Write an inference algorithms that makes use of hardware acceleration
				in video cards.
		\item	Write a parallel inference algorithm for the LDA model that
				can be run on clusters.
	\end{enumerate}
	
	
	
\section{Tasks}


	\subsection{Theory}
	
		In order to place LDA in its historical context I think I should first learn about LSA, its 
		precursor.
		Then, I want to re-read the latest version of the LDA review paper by Gregor Heinrich \cite{heinrich2005parameter},
		which is an amazing resume and discussion of LDA from first principles and with lots of details.
		Also I want to learn about the exponential family in general and more background material
		about the variational inference principles  \cite{wainwright2008graphical}.
		
		\vspace{3in}		
		
		
		
	\subsection{Topic maps}
	
		distance between topics of the same run \\
		visualization ?
		
		\vspace{3in}
	
	\subsection{Consistency}
		
		Evaluation of topic models have been difficult. 
		\bigskip
		\bigskip
		
		How similar are the topics extracted from a document collection
		between different runs of the inference algorithms.
		Are the topics extracted similar for Gibbs sampling and CVB based
		inference ?
		Play with $\alpha, \beta$ parameters on priors \cite{heinrich2005parameter,wallach-rethinking}.
		
		To study this we want to repeat the approach in [ $??$ ] which 
		performs the Hungarian-algorithm for matching on the topics 
		extracted from different runs.
		%
		They claim to have about 80\% of topics match between different runs.
		
				\vspace{3in}
		
	\subsection{Hierarchical topics}
	
		topic subtopic relationship ...
		
				\vspace{3in}
						\vspace{3in}
						

	\subsection{\texttt{liblda.py}}
	
		In order to run the experiments for ..
		pre-processing in python 
		LDA C programs by Newman  \cite{NewmanCode}.
		
		If I am going to be working on this project I need to cleanup the code.
		I want to refactor the whole
		code base into a self contained python library \texttt{liblda} with a simple API.
		
		\cite{IvanCode}.
		
		%import from other write up
	    
	    The user writes edits a config file which explains where the document collection lives
	    and then the uses two-three simple commands to run an LDA experiment.

	    The results will be available (they already are kind of) as numpy arrays, so that
	    more complex algorithms can be build on top of the basic LDA operations.

	    I want to follow the ideas of "loose coupling" and allow to swap out components like
	    the underlying inference algorithm.


	
				\vspace{3in}
				
						\vspace{3in}
						
						
		
		
	\subsection{Parallel LDA}

	        One aspect that I did not get a chance to look into deeper is the use of parallel algorithms
	        for inference of the LDA model.
	        One approach is to modify the Gibbs sampling or variational methods to permit loosely
	        coupled parallel operation \cite{newman2006scalable,newman2007distributed}. 
	        
	        %Other results about come 
	        %\cite{chang2007psvm}
	        		\vspace{3in}
					\vspace{3in}
					

	\subsection{LDA on the GPU}
		
	        Another approach is to use GPU based computation which is highly parallel and vectorized.
	        CUDA (Compute Unified Device Architecture) is a simple API which allows for GPU programming
	        on NVIDIA grpahics boards. The speedups reported are very impressive
	        \cite{masada2009accelerating, yan-parallel}.
	        %This research direction would also be an excuse for me to buy an expensive graphics card.


				\vspace{3in}
						\vspace{3in}
						

\section{Other less clear ideas}


	\subsection{Regular expressions counts}
        
        In text analysis we invariably use word counts as the main ``features'' of documents.
        Since we have the full power and flexibility of python during the pre-processing stage,
        we could use instead regular expressions as features.

        For example we could quantify how many matches to the reg ex ``\texttt{H(.*)}'' there are
        in each document. Certainly, for papers that deal with information theory this will be 
        a highly informative feature.

		%We can have higher level features counts from regular expression matches.
		%These features can themselves be refined over generations or by cross validation.
		%Come to think of it you can do all kinds of things with this.
        Another possible use of regular expressions is to have a digram model with synonyms
        taken into account. For example we could count how many matches there are to this
        reglar expression ``\texttt{quantum (mechanics|physics)}'' instead of trying to keep
        track of the two options separately.
		
		%Algorithm: Self-refining regular expression features
		%\begin{enumerate}
		%	\item	Start with word counts for a fixed list of words of size $|W|$ and do regular LDA.
		%	\item	For each of the latent topics build all "two word" combinations.
		%	\item	Rerun LDA on the expanded feature space of size $|W|^2 + |W|$.
		%	\item	Find the important second order occurrences and discard all other features.
		%	\item	Try to compress multiple variations using regular expression language. \\
		%			Example: "a b", "a c", "a d"  can be converted into regex "a (b|c|d)"
		%	\item	Go to step 3 but now number of features is $|W|+$ n\_of\_good\_reg\_exes
		%\end{enumerate}

		%I am not sure if this will generate all possible word occurrences but I think reg-exes are
		%pretty rich as feature set and their computation is all parallelizable so it doesn't matter if 
		%it is very expensive.
		
		%Can we let some genetic algorithm work on this instead?


    \subsection{Two data set correlations}

        I have not seen in the literature any discussion about using LDA to automatically
        create links between two data collections.
        For example if we trained a topic model on wikipedia articles AND the arXiv,
        we would be able to automatically generate meaningful links between the two.
        As you browse the arXiv you could get suggested articles from wikipedia that cover similar topics.


			
    \subsection{Citation graph}

        One simple further step that can be taken to make use of the arXiv topic model 
        is to extract the citation graph amongst the paper. This can be done with another
        regular expression of the form \verb|quant-ph\d{7}| or from some third party
        source like Citeseer.
        
        Having the citaiton graph would allows us to do a ``page rank'' type of calculation
        and extract a sense of importance for the papers in each topic.


    \subsection{Metadata}
        We should have the titles for each of these articles I think
        we need a little urllib2 script to get each of
        \verb|http://arxiv.org/list/quant-ph/0003?show=1000|
        where 0003 corresponds to my folder 0003 


    %Calculate perplexity and KL distance for different runs with same parameters
    %\cite{newman2006scalable}.
    
    \vspace{0.3in}

    I feel this is more of a beginning than an end to this project.
    I have gotten acquainted with the software and the data set and hopefully I can produce
    some worthwhile results in the coming months.
    All the source code and results associated with this project are available here \cite{IvanCode}.


\section{Applications}

	In order to show off the power of LDA there should be
	
	\subsection{ArXiv browser}
	
	    In particular I would like to use the extracted topics in order to develop an automated
	    recommendation system or a ``topic filter'' which would help me keep track of which papers
	    are posted on the arXiv every day that are relevant to my topics of research.

	   Other approaches for visualization and evaluation of topic models are suggested in 
	    \cite{boyd2009reading}. % reading tea leaves....

		\vspace{3in}	    
	   
	\subsection{Topic extraction as a service}
	
		Use our word counter and vocabulary picker, then just send us your count vectors.
		preserves privacy

		\vspace{3in}

	\subsection{Autotagging}
	
		run topic model, \\
		present to user for him to tag topics, \\
		apply these tags systemwide to all documents

		\vspace{3in}
				
				
	

	%\subsection{Paper cover}
	%
	%	User interface idea: papers laid out on a table in such a way that the papers that
	%	cite each other overlap. All papers in the same "epoch" are visible at the same time
	%	and placement and paper size indicates relative importance.
	%	
	%	(One way to think about this is where are the words of this each topic passed on from one generation
	%	to the next)
		

\pagebreak

	

\bibliographystyle{alpha}
\bibliography{arXivLDA}


\end{document}
